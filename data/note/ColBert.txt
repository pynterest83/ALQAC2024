The content of Vietnamese law articles is notably lengthy and follows a specific structure. This is shown in Figure \ref{fig:train} and Figure \ref{fig:law}. Although the aforementioned contrastive ranking model utilizing S-BERT pretrained on a Vietnamese dataset is appropriate for the Vietnamese language, the majority of articles are approximately 300 tokens long, exceeding the 256-token limit of the pre-trained model.

Another factor contributing to suboptimal ranking results with the contrastive model is that the pre-trained S-BERT model, fine-tuned based on RoBERTa \cite{liu2019roberta} for Vietnamese data, follows the same Masked Language Modeling (MLM) approach as BERT \cite{devlin-etal-2019-bert} which is not effective for retrieval. Despite further fine-tuning the model in our dataset and employing contrastive learning to enhance retrieval capability, the results were not satisfactory.

Upon identifying that the pre-trained model was inadequate, we adopted ColBERTv2 \cite{santhanam2021colbertv2} based on ColBERT \cite{khattab2020colbert}, a model employs several advanced techniques to achieve state-of-the-art performance in retrieval and search tasks. Central to its design is the late interaction model, where queries and documents are encoded independently, and their interaction occurs post-encoding. This approach optimizes efficiency and scalability by enabling the pre-computation of document embedding, which significantly speeds up the retrieval process. Denoised supervision further enhances training by incorporating challenging negatives and leveraging cross-encoder distillation. ColBERTv2's indexing strategy encodes documents into bags of embedding, one per token, and utilizes FAISS for efficient indexing and retrieval. Moreover, the pre-trained model supports a maximum input length of 512 tokens, effectively addressing the length limitation issue. These techniques collectively ensure ColBERTv2's retrieval speed and efficiency, as pre-computed document embedding allow for rapid query comparisons, and only the query needs to be encoded at inference, reducing computational demands.